{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411a3ad4-e63f-4376-bf3f-41c60d4a16f6",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7991467-baee-4f38-8881-2b19c848bda6",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7996d3-0f66-4506-8f2a-132b46856105",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "- Shape: 506 rows × 14 columns\n",
    "- Target column: MEDV = median value of owner-occupied homes (in $1,000s)\n",
    "#### Missing values\n",
    "- Some columns had missing values (example: CRIM had only 486 non-null values out of 506).\n",
    "- I handled missing values using mean imputation per numeric column because linear models don’t accept NaNs, and mean imputation is a very easy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e659c43-2c6d-4bbe-872d-e11dce6c4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971d905a-9ef0-4c14-9422-9e2a6734a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearRegressionGD:\n",
    "    \"\"\"\n",
    "    custom linear regression trained with batch gradient descent.\n",
    "\n",
    "    theory (from Sigmoid ML Handbook):\n",
    "    we assume a linear model y_hat = Xw, and we want to find weights w\n",
    "    that minimize the sum of squared errors:\n",
    "        f(w) = (Xw - y)^T (Xw - y)\n",
    "    the gradient of this cost wrt w is:\n",
    "        grad = 2 * X^T (Xw - y)\n",
    "    we iteratively update:\n",
    "        w <- w - alpha * grad\n",
    "    until convergence / fixed iterations.\n",
    "\n",
    "    other tweaks:\n",
    "    -feature standardization (mean 0, std 1) for stability.\n",
    "    -explicit bias term (a column of 1s).\n",
    "    -deterministic random_state for reproducible initialization.\n",
    "    -score() method that returns R^2, so it matches sklearn's .score().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=5000, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "\n",
    "        #will be learned in fit()\n",
    "        self.w_ = None      #weight vector INCLUDING bias term at the end\n",
    "        self.mean_ = None   #per-feature mean\n",
    "        self.std_ = None    #per-feature std (for scaling)\n",
    "\n",
    "    def _prepare_features_fit(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using training data stats and\n",
    "        append a bias column of 1s.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "\n",
    "        #compute and store scaling parameters\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.std_ = X.std(axis=0)\n",
    "        #avoid division by zero if a column is constant\n",
    "        self.std_[self.std_ == 0] = 1.0\n",
    "\n",
    "        #standardize\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "\n",
    "        #add bias column (all 1s) as last column\n",
    "        ones = np.ones((X_scaled.shape[0], 1))\n",
    "        X_aug = np.concatenate([X_scaled, ones], axis=1)\n",
    "\n",
    "        #initialize weights randomly (small values)\n",
    "        self.w_ = rng.normal(loc=0.0, scale=0.01, size=(X_aug.shape[1], 1))\n",
    "\n",
    "        return X_aug\n",
    "\n",
    "    def _prepare_features_predict(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using the *stored* mean_ and std_,\n",
    "        then append bias column of 1s (same trick as in training).\n",
    "        \"\"\"\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "        ones = np.ones((X_scaled.shape[0], 1))\n",
    "        X_aug = np.concatenate([X_scaled, ones], axis=1)\n",
    "        return X_aug\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit the model using full-batch gradient descent.\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Training features.\n",
    "        y : array-like, shape (n_samples,) or (n_samples, 1)\n",
    "            Target values (MEDV in our case).\n",
    "        \"\"\"\n",
    "        #prepare features (scaling + bias) and ensure y is column vector\n",
    "        X_aug = self._prepare_features_fit(X)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        n_samples = X_aug.shape[0]\n",
    "\n",
    "        #gradient Descent loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            #forward pass: predictions\n",
    "            y_pred = X_aug @ self.w_\n",
    "\n",
    "            #gradient of MSE-like objective \n",
    "            gradient = (2.0 / n_samples) * (X_aug.T @ (y_pred - y))\n",
    "\n",
    "            #update rule: w <- w - alpha * grad\n",
    "            self.w_ -= self.learning_rate * gradient\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict using learned weights.\n",
    "        \"\"\"\n",
    "        X_aug = self._prepare_features_predict(X)\n",
    "        return X_aug @ self.w_\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        compute R^2, same definition as sklearn's LinearRegression.score.\n",
    "        R^2 = 1 - SS_res / SS_tot\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1, 1)\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "        return r2\n",
    "\n",
    "\n",
    "def load_and_clean_data(zip_path):\n",
    "    \"\"\"\n",
    "    load HousingData.csv from the given ZIP file, fill missing numeric values\n",
    "    with column means, and return the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        #find CSV inside zip\n",
    "        csv_files = [f for f in z.namelist() if f.endswith(\".csv\")]\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(\"no CSV file found in archive.\")\n",
    "        data_file = csv_files[0]\n",
    "\n",
    "        with z.open(data_file) as f:\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "    #mean imputation for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_correlations(df, target_col=\"MEDV\"):\n",
    "    \"\"\"\n",
    "    compute Pearson correlation with the target column,\n",
    "    return a DataFrame sorted by absolute correlation descending.\n",
    "    \"\"\"\n",
    "    corr_matrix = df.corr(numeric_only=True)\n",
    "    #drop target itself\n",
    "    target_corr = corr_matrix[target_col].drop(target_col)\n",
    "\n",
    "    corr_df = pd.DataFrame({\n",
    "        \"feature\": target_corr.index,\n",
    "        \"corr_with_MEDV\": target_corr.values,\n",
    "        \"abs_corr\": np.abs(target_corr.values)\n",
    "    }).sort_values(by=\"abs_corr\", ascending=False)\n",
    "\n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def select_feature_subset(corr_df, low=0.5, high=0.8):\n",
    "    \"\"\"\n",
    "    select feature names whose absolute correlation with the target is\n",
    "    between 'low' and 'high', inclusive.\n",
    "    \"\"\"\n",
    "    mask = (corr_df[\"abs_corr\"] >= low) & (corr_df[\"abs_corr\"] <= high)\n",
    "    return corr_df.loc[mask, \"feature\"].tolist()\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    train both sklearn's LinearRegression and our CustomLinearRegressionGD\n",
    "    on the provided train/test split. return their R^2 scores.\n",
    "    \"\"\"\n",
    "    #Sklearn OLS \n",
    "    skl_lr = LinearRegression()\n",
    "    skl_lr.fit(X_train, y_train)\n",
    "    r2_skl = skl_lr.score(X_test, y_test)\n",
    "\n",
    "    #Custom GD Linear Regression\n",
    "    custom_lr = CustomLinearRegressionGD(\n",
    "        learning_rate=0.01,\n",
    "        n_iterations=5000,\n",
    "        random_state=42\n",
    "    )\n",
    "    custom_lr.fit(X_train, y_train)\n",
    "    r2_custom = custom_lr.score(X_test, y_test)\n",
    "\n",
    "    return r2_skl, r2_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76d7c4a-b759-4ced-835c-6708283bb715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with MEDV (sorted by |corr|):\n",
      "     feature  corr_with_MEDV  abs_corr\n",
      "12    LSTAT       -0.721975  0.721975\n",
      "5        RM        0.695360  0.695360\n",
      "10  PTRATIO       -0.507787  0.507787\n",
      "2     INDUS       -0.478657  0.478657\n",
      "9       TAX       -0.468536  0.468536\n",
      "4       NOX       -0.427321  0.427321\n",
      "8       RAD       -0.381626  0.381626\n",
      "6       AGE       -0.380223  0.380223\n",
      "0      CRIM       -0.379695  0.379695\n",
      "1        ZN        0.365943  0.365943\n",
      "11        B        0.333461  0.333461\n",
      "7       DIS        0.249929  0.249929\n",
      "3      CHAS        0.179882  0.179882 \n",
      "\n",
      "Selected high-corr features (|corr| in [0.5, 0.8]): ['LSTAT', 'RM', 'PTRATIO']\n",
      "\n",
      "=== Final R^2 comparison table ===\n",
      "dataset       model  R2_test\n",
      "   full sklearn_OLS 0.658852\n",
      "   full   custom_GD 0.658835\n",
      " subset sklearn_OLS 0.624552\n",
      " subset   custom_GD 0.624552\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #load + clean data\n",
    "    df = load_and_clean_data(\"data/archive.zip\")\n",
    "\n",
    "    #correlation analysis\n",
    "    corr_df = compute_correlations(df, target_col=\"MEDV\")\n",
    "    print(\"Correlation with MEDV (sorted by |corr|):\\n\", corr_df, \"\\n\")\n",
    "\n",
    "    #prepare full and subset feature matrices\n",
    "    target = \"MEDV\"\n",
    "    X_full = df.drop(columns=[target]).values #all predictors\n",
    "    y_full = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    subset_features = select_feature_subset(corr_df, low=0.5, high=0.8)\n",
    "    print(\"Selected high-corr features (|corr| in [0.5, 0.8]):\", subset_features)\n",
    "\n",
    "    X_sub = df[subset_features].values #only top drivers\n",
    "    y_sub = y_full\n",
    "\n",
    "    #train/test split for BOTH datasets\n",
    "    X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "        X_full, y_full, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "        X_sub, y_sub, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    #train & evaluate on full feature set\n",
    "    r2_full_skl, r2_full_custom = train_and_evaluate_models(\n",
    "        X_train_full, X_test_full, y_train_full, y_test_full\n",
    "    )\n",
    "\n",
    "    #train & evaluate on subset feature set\n",
    "    r2_sub_skl, r2_sub_custom = train_and_evaluate_models(\n",
    "        X_train_sub, X_test_sub, y_train_sub, y_test_sub\n",
    "    )\n",
    "\n",
    "    #summarize results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"dataset\": [\"full\", \"full\", \"subset\", \"subset\"],\n",
    "        \"model\": [\"sklearn_OLS\", \"custom_GD\", \"sklearn_OLS\", \"custom_GD\"],\n",
    "        \"R2_test\": [r2_full_skl, r2_full_custom, r2_sub_skl, r2_sub_custom]\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== Final R^2 comparison table ===\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5f6e2-6fff-4ba8-a14d-0ed3efa35341",
   "metadata": {},
   "source": [
    "**R² measures how much variance in the target is explained by the model. \n",
    "1.0 = perfect, 0.0 = predicts just the mean of y, negative = worse than mean.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055dce9-f817-432a-90d8-62b58eb48fd9",
   "metadata": {},
   "source": [
    "#### Features with the strongest relationship to price\n",
    "Top 3 by absolute correlation:\n",
    "1. LSTAT (-0.72)\n",
    "- LSTAT = % of lower-status population in the area.\n",
    "- Higher LSTAT - lower house prices.\n",
    "- Business intuition: Neighborhood socioeconomic profile affects perceived safety, amenities, reputation, demand - directly reflected in property values.\n",
    "2. RM (+0.70)\n",
    "- RM = avg number of rooms per dwelling.\n",
    "- More rooms → larger, more comfortable houses → higher price. This is literally “bigger house = more money,” which makes sense.\n",
    "3. PTRATIO (-0.51)\n",
    "- PTRATIO = pupil–teacher ratio in local schools.\n",
    "- Higher PTRATIO = more students per teacher (more crowded schools, lower perceived education quality).\n",
    "- Worse schools → less attractive to families → lower property values.\n",
    "\n",
    "Real Estate POV:\n",
    "- People are willing to pay more for bigger houses (RM↑).\n",
    "- People are willing to pay more to avoid social/economic disadvantage (LSTAT↓).\n",
    "- People are willing to pay more for good schools (PTRATIO↓)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14784b7d-c59f-46d8-8837-584aa9e68a91",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "1. Scratch implementation basically matches sklearn.\n",
    "The R² values for custom_GD and sklearn_OLS are nearly identical on both datasets.\n",
    "2. The 3-feature model is surprisingly strong.\n",
    "Using only LSTAT, RM, PTRATIO I still got ~0.625 R², which is only slightly worse than the ~0.659 R² I got with the full feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fbf5e-94c7-46b4-b9b6-3b366d1cab0e",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4982aada-c287-4cba-9ed3-92824122d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogReg\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853df591-739e-4104-9980-bafd2459ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (303, 14)\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n",
      "None\n",
      "\n",
      "Missing BEFORE:\n",
      " age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Missing AFTER:\n",
      " age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Correlation with target (sorted by |corr|):\n",
      " feature  corr_with_target  abs_corr\n",
      "   exang         -0.436757  0.436757\n",
      "      cp          0.433798  0.433798\n",
      " oldpeak         -0.430696  0.430696\n",
      " thalach          0.421741  0.421741\n",
      "      ca         -0.391724  0.391724\n",
      "   slope          0.345877  0.345877\n",
      "    thal         -0.344029  0.344029\n",
      "     sex         -0.280937  0.280937\n",
      "     age         -0.225439  0.225439\n",
      "trestbps         -0.144931  0.144931\n",
      " restecg          0.137230  0.137230\n",
      "    chol         -0.085239  0.085239\n",
      "     fbs         -0.028046  0.028046\n",
      "\n",
      "Top 5 most correlated features:\n",
      "feature  corr_with_target  abs_corr\n",
      "  exang         -0.436757  0.436757\n",
      "     cp          0.433798  0.433798\n",
      "oldpeak         -0.430696  0.430696\n",
      "thalach          0.421741  0.421741\n",
      "     ca         -0.391724  0.391724\n",
      "\n",
      "No features in [0.5, 0.8] corr range. Using top 3 instead.\n",
      "Selected subset features: ['exang', 'cp', 'oldpeak']\n"
     ]
    }
   ],
   "source": [
    "#load and clean data\n",
    "DATA_PATH = \"data/heart.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "#basic info\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "#handle missing values by mean imputation for numeric columns\n",
    "missing_before = df.isna().sum()\n",
    "print(\"\\nMissing BEFORE:\\n\", missing_before)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "missing_after = df.isna().sum()\n",
    "print(\"\\nMissing AFTER:\\n\", missing_after)\n",
    "\n",
    "#sanity: target should be int 0/1\n",
    "df[\"target\"] = df[\"target\"].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#correlation analysis with target\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "target_corr = corr_matrix[\"target\"].drop(\"target\").sort_values(\n",
    "    key=lambda x: x.abs(), ascending=False\n",
    ")\n",
    "\n",
    "corr_df = pd.DataFrame({\n",
    "    \"feature\": target_corr.index,\n",
    "    \"corr_with_target\": target_corr.values,\n",
    "    \"abs_corr\": np.abs(target_corr.values)\n",
    "})\n",
    "\n",
    "print(\"\\nCorrelation with target (sorted by |corr|):\")\n",
    "print(corr_df.to_string(index=False))\n",
    "\n",
    "#for interprtstion\n",
    "print(\"\\nTop 5 most correlated features:\")\n",
    "print(corr_df.head(5).to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "#feature subset selection\n",
    "#    requirement: abs corr in [0.5, 0.8].\n",
    "#    this dataset: no feature is that strong (~0.44 max).\n",
    "#    choose top 3 strongest features.\n",
    "\n",
    "\n",
    "subset_mask = (corr_df[\"abs_corr\"] >= 0.5) & (corr_df[\"abs_corr\"] <= 0.8)\n",
    "subset_features = corr_df.loc[subset_mask, \"feature\"].tolist()\n",
    "\n",
    "if len(subset_features) == 0:\n",
    "    # so we don't end up with 0 features\n",
    "    subset_features = corr_df.head(3)[\"feature\"].tolist()\n",
    "    print(\"\\nNo features in [0.5, 0.8] corr range. Using top 3 instead.\")\n",
    "else:\n",
    "    print(\"\\nUsing features in [0.5, 0.8] corr range.\")\n",
    "\n",
    "print(\"Selected subset features:\", subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972540ef-5838-4c74-b159-b071e070cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full feature split shapes:\n",
      "X_train_full: (242, 13)\n",
      "X_test_full : (61, 13)\n",
      "y_train_full: (242,)\n",
      "y_test_full : (61,)\n",
      "\n",
      "Subset feature split shapes:\n",
      "X_train_sub: (242, 3)\n",
      "X_test_sub : (61, 3)\n",
      "y_train_sub: (242,)\n",
      "y_test_sub : (61,)\n"
     ]
    }
   ],
   "source": [
    "#train/test splits\n",
    "#full feature set\n",
    "X_full = df.drop(columns=[\"target\"]).values\n",
    "y_full = df[\"target\"].values  #0 = disease, 1 = no disease\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFull feature split shapes:\")\n",
    "print(\"X_train_full:\", X_train_full.shape)\n",
    "print(\"X_test_full :\", X_test_full.shape)\n",
    "print(\"y_train_full:\", y_train_full.shape)\n",
    "print(\"y_test_full :\", y_test_full.shape)\n",
    "\n",
    "#subset feature set\n",
    "X_sub = df[subset_features].values\n",
    "y_sub = y_full\n",
    "\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "    X_sub, y_sub, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSubset feature split shapes:\")\n",
    "print(\"X_train_sub:\", X_train_sub.shape)\n",
    "print(\"X_test_sub :\", X_test_sub.shape)\n",
    "print(\"y_train_sub:\", y_train_sub.shape)\n",
    "print(\"y_test_sub :\", y_test_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a427df-6d58-4d38-812a-3100ca5137ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn Logistic Regression accuracy:\n",
      "Full features (model.score):        0.8852459016393442\n",
      "Full features (accuracy_score):     0.8852459016393442\n",
      "Subset features (model.score):      0.819672131147541\n",
      "Subset features (accuracy_score):   0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "#sklearn Logistic Regression\n",
    "sk_full = SklearnLogReg(max_iter=10000)\n",
    "sk_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "sk_sub = SklearnLogReg(max_iter=10000)\n",
    "sk_sub.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "#sklearn's .score() for classifiers\n",
    "acc_full_sklearn_score = sk_full.score(X_test_full, y_test_full)\n",
    "acc_sub_sklearn_score = sk_sub.score(X_test_sub, y_test_sub)\n",
    "\n",
    "#also verify with accuracy_score\n",
    "acc_full_sklearn = accuracy_score(y_test_full, sk_full.predict(X_test_full))\n",
    "acc_sub_sklearn = accuracy_score(y_test_sub, sk_sub.predict(X_test_sub))\n",
    "\n",
    "print(\"\\nSklearn Logistic Regression accuracy:\")\n",
    "print(\"Full features (model.score):       \", acc_full_sklearn_score)\n",
    "print(\"Full features (accuracy_score):    \", acc_full_sklearn)\n",
    "print(\"Subset features (model.score):     \", acc_sub_sklearn_score)\n",
    "print(\"Subset features (accuracy_score):  \", acc_sub_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d99082-6212-4dfe-a77c-16c940af7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5240\\1839241889.py:27: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Logistic Regression accuracy:\n",
      "Full features accuracy:    0.8524590163934426\n",
      "Subset features accuracy:  0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "#custom Logistic Regression (Gradient Descent)\n",
    "#    matches the SMLH approach:\n",
    "#    - linear score -> sigmoid -> probability (binary)\n",
    "#    - log-loss cost\n",
    "#    - batch gradient descent weight update\n",
    "class CustomLogisticRegressionGD:\n",
    "    \"\"\"\n",
    "    bare-bones Logistic Regression trained with batch Gradient Descent.\n",
    "\n",
    "    - y_hat = sigmoid(X @ w)\n",
    "    - loss = log loss (cross-entropy)\n",
    "    - w <- w - alpha * gradient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.05, max_iter=100000):\n",
    "        self.__learning_rate = learning_rate\n",
    "        self.__max_iter = max_iter\n",
    "        #coef_ will store weights INCLUDING the bias at the end\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"append a column of 1s to learn the bias term explicitly.\"\"\"\n",
    "        return np.hstack((X, np.ones((len(X), 1))))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Standard logistic sigmoid: 1 / (1 + exp(-z)).\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using full-batch gradient descent.\n",
    "        X: shape (n_samples, n_features)\n",
    "        y: shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X_aug = self._add_intercept(X)\n",
    "\n",
    "        # init weights to zeros (n_features + 1, because of bias)\n",
    "        self.coef_ = np.zeros(X_aug.shape[1])\n",
    "\n",
    "        for _ in range(self.__max_iter):\n",
    "            # predicted probability of class 1\n",
    "            pred = self.sigmoid(np.dot(X_aug, self.coef_))\n",
    "\n",
    "            # gradient of average log loss w.r.t. weights\n",
    "            gradient = np.dot(X_aug.T, (pred - y)) / y.size\n",
    "\n",
    "            # gradient descent step\n",
    "            self.coef_ -= self.__learning_rate * gradient\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        return probabilities for [class0, class1] for each row.\n",
    "        \"\"\"\n",
    "        X_aug = self._add_intercept(X)\n",
    "        prob_1 = self.sigmoid(np.dot(X_aug, self.coef_))\n",
    "        prob_0 = 1.0 - prob_1\n",
    "        return np.vstack((prob_0, prob_1)).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        convert probabilities to hard class labels using a threshold.\n",
    "        \"\"\"\n",
    "        X_aug = self._add_intercept(X)\n",
    "        prob_1 = self.sigmoid(np.dot(X_aug, self.coef_))\n",
    "        return (prob_1 > threshold).astype(int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        classification accuracy, same definition as sklearn's .score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "#train custom models\n",
    "custom_full = CustomLogisticRegressionGD(learning_rate=0.05, max_iter=100000)\n",
    "custom_full.fit(X_train_full, y_train_full)\n",
    "acc_full_custom = custom_full.score(X_test_full, y_test_full)\n",
    "\n",
    "custom_sub = CustomLogisticRegressionGD(learning_rate=0.05, max_iter=100000)\n",
    "custom_sub.fit(X_train_sub, y_train_sub)\n",
    "acc_sub_custom = custom_sub.score(X_test_sub, y_test_sub)\n",
    "\n",
    "print(\"\\nCustom Logistic Regression accuracy:\")\n",
    "print(\"Full features accuracy:   \", acc_full_custom)\n",
    "print(\"Subset features accuracy: \", acc_sub_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e450f1-b59e-41ce-bd26-61d61feabebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final accuracy comparison table ===\n",
      "dataset            model  accuracy_test\n",
      "   full   sklearn_LogReg       0.885246\n",
      "   full custom_GD_LogReg       0.852459\n",
      " subset   sklearn_LogReg       0.819672\n",
      " subset custom_GD_LogReg       0.819672\n"
     ]
    }
   ],
   "source": [
    "#final comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    \"dataset\": [\"full\", \"full\", \"subset\", \"subset\"],\n",
    "    \"model\": [\"sklearn_LogReg\", \"custom_GD_LogReg\", \"sklearn_LogReg\", \"custom_GD_LogReg\"],\n",
    "    \"accuracy_test\": [\n",
    "        acc_full_sklearn, acc_full_custom,\n",
    "        acc_sub_sklearn, acc_sub_custom\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Final accuracy comparison table ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401473cc-40eb-4f7a-9061-b31a4f3ec7da",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "- I handled data, imputed, analyzed correlations, chose a most predictive subset.\n",
    "- I trained both sklearn LogisticRegression and gradient-descent Logistic Regression (sigmoid activation, log loss, gradient descent updates). \n",
    "- on the full feature set it reached ~86.9% test accuracy (sklearn), ~83.6% (custom).\n",
    "- on the 3-feature subset (exang, cp, oldpeak) it still got ~82% accuracy.\n",
    "- the top clinical signals: chest pain type, ECG changes under stress, and angina during exercise are highly predictive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
